apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: cme-datacuration-
  namespace: argo
spec:
  entrypoint: bigquery-join
  serviceAccountName: argo-events-ksa
  templates:
    - name: bigquery-join
      steps:
        - - name: process-pubsub-msg
            template: pubsub-msg-processing
            arguments:
              parameters:
                - name: pubsub_msg
                  value: "{{workflow.parameters.pubsub_msg}}"
        - - name: check-processing-zone
            template: check-table-data
            arguments:
              parameters:
                - name: partition_column
                  value: "{{workflow.parameters.partition_column}}"
                - name: processing_zone_table
                  value: "{{workflow.parameters.processing_zone_table}}"
                - name: partition_value #will be ingested from pubsub event sensor
                  value: "{{workflow.parameters.partition_value}}"
        - - name: run-join
            template: append-flat-table
            arguments:
              parameters:
                - name: table1
                  value: "{{workflow.parameters.table1}}"
                - name: table2
                  value: "{{workflow.parameters.table2}}"
                - name: column1
                  value: "{{workflow.parameters.column1}}"
                - name: column2
                  value: "{{workflow.parameters.column2}}"
                - name: join_condition
                  value: "{{workflow.parameters.join_condition}}"
                - name: destination_dataset
                  value: "{{workflow.parameters.destination_dataset}}"
                - name: destination_table
                  value: "{{workflow.parameters.destination_table}}"
        - - name: create-curated-zone-view
            template: create-cz-view
            arguments:
              parameters:
                - name: destination_dataset
                  value: "{{workflow.parameters.destination_dataset}}"
                - name: destination_table
                  value: "{{workflow.parameters.destination_table}}"
                - name: view_name
                  value: "{{workflow.parameters.view_name}}"

    - name: append-flat-table
      inputs:
        parameters:
          - name: table1
          - name: table2
          - name: column1
          - name: column2
          - name: join_condition
          - name: destination_dataset
          - name: destination_table
      script:
        image: us-central1-docker.pkg.dev/gke-expeiments/cme-setup/testimage:latest
        command: [python]
        resources:
          requests:
            cpu: 0.5
            memory: 100Mi
        source: |
          from google.cloud import bigquery
          client = bigquery.Client()
          
          table1 = "{{inputs.parameters.table1}}"
          table2 = "{{inputs.parameters.table2}}"
          column1 = "{{inputs.parameters.column1}}"
          column2 = "{{inputs.parameters.column2}}"
          join_condition = "{{inputs.parameters.join_condition}}"
          destination_dataset = "{{inputs.parameters.destination_dataset}}"
          destination_table = "{{inputs.parameters.destination_table}}"
          
          query = f"SELECT t1.GUID, t2.UNDERLYING_GUID, t2.option_type, t1.run_date FROM `{table1}` t1 JOIN `{table2}` t2 ON t1.`{column1}` = t2.`{column2}` AND {join_condition}"
          write_disposition = bigquery.WriteDisposition.WRITE_APPEND 
          table_ref = client.dataset(destination_dataset).table(destination_table)
          job_config = bigquery.QueryJobConfig(destination=table_ref, write_disposition=write_disposition)
          query_job = client.query(query, job_config=job_config)
          results = query_job.result()
          
          for row in results:
            print(row)
    - name: create-cz-view
      inputs:
        parameters:
          - name: destination_dataset
          - name: destination_table
          - name: view_name
      script:
        image: us-central1-docker.pkg.dev/gke-expeiments/cme-setup/testimage:latest
        command: [python]
        resources:
          requests:
            cpu: 0.5
            memory: 100Mi
        source: |
          from google.cloud import bigquery, exceptions
          client = bigquery.Client()
          
          destination_dataset = "{{inputs.parameters.destination_dataset}}"
          destination_table = "{{inputs.parameters.destination_table}}"
          view_name = "{{inputs.parameters.view_name}}"
          view_ref = client.dataset(destination_dataset).table(view_name)
          
          view_query = f"SELECT GUID, UNDERLYING_GUID, option_type, run_date FROM `{destination_dataset}.{destination_table}` "
          try:
            view_creation_obj = bigquery.Table(view_ref)
            view_creation_obj.view_query = view_query
            view_creation_obj.view_use_legacy_sql = False
            client.create_table(view_creation_obj)
          except Exception as e:
            print("Either the view already exists or some other error ")
    - name: check-table-data
      inputs:
        parameters:
          - name: partition_column
          - name: partition_value
          - name: processing_zone_table
          - name: ingested_rundate
            value: "{{workflow.outputs.parameters.run_date}}"
      script:
        image: us-central1-docker.pkg.dev/gke-expeiments/cme-setup/testimage:latest
        command: [python]
        resources:
          requests:
            cpu: 0.5
            memory: 100Mi
        source: |
          from google.cloud import bigquery
          client = bigquery.Client()
          
          partition_column = "{{inputs.parameters.partition_column}}"
          partition_value = "{{inputs.parameters.partition_value}}"
          processing_zone_table = "{{inputs.parameters.processing_zone_table}}"
          ingested_run_date = "{{inputs.parameters.ingested_rundate}}"
          print('{{inputs.parameters.ingested_rundate}}')
          
          query = f"SELECT COUNT(1) FROM `{processing_zone_table}` where {partition_column}='{ingested_run_date}'"
          print(query)
          query_job = client.query(query)
          results = query_job.result()
          
          if results:
            row = next(results)
            num_rows = row[0]
            if num_rows > 0:
              print(f"Processing zone Table '{processing_zone_table}' has records.")
              delete_query = f"DELETE FROM `{processing_zone_table}` where {partition_column}='{partition_value}'"
              delete_query_job = client.query(delete_query)
            else:
              print(f"Processing zone table is empty.")
          else:
            print(f"Error: Couldn't retrieve data from processing zone table.")
    - name: pubsub-msg-processing
      inputs:
        parameters:
          - name: pubsub_msg
      script:
        image: us-central1-docker.pkg.dev/gke-expeiments/cme-setup/testimage:latest
        command: [python]
        resources:
          requests:
            cpu: 0.5
            memory: 100Mi
        source: |
          from google.cloud import bigquery
          import json
          import os
          
          output_file_path = "/tmp/run_date.txt"
          pubsub_msg = '{{inputs.parameters.pubsub_msg}}'
          msg_obj = json.loads(pubsub_msg)
          print(msg_obj["run_date"])
          run_date = msg_obj["run_date"]
          with open(output_file_path, "w") as file:
            print("Writing run_Date to output file")
            file.write(run_date)
      outputs:
        parameters:
          - name: run_date
            valueFrom:
              path: /tmp/run_date.txt
            globalName: run_date
